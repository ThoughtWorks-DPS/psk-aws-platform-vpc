<div align="center">
	<p>
	<img alt="Thoughtworks Logo" src="https://raw.githubusercontent.com/ThoughtWorks-DPS/static/master/thoughtworks_flamingo_wave.png?sanitize=true" width=200 /><br />
	<img alt="DPS Title" src="https://raw.githubusercontent.com/ThoughtWorks-DPS/static/master/EMPCPlatformStarterKitsImage.png?sanitize=true" width=350/><br />
	<h2>psk-aws-iam-profiles</h2>
	<a href="https://opensource.org/licenses/MIT"><img src="https://img.shields.io/github/license/ThoughtWorks-DPS/lab-iam-profiles"></a> <a href="https://aws.amazon.com"><img src="https://img.shields.io/badge/-deployed-blank.svg?style=social&logo=amazon"></a>
	</p>
</div>

## maintainers notes

### architecture  

_vpc subnet structure_  

A typical single-cluster vpc definition:  

In this example, the private-ingress range represents an allocation from what would be the enterprise shared network space. Load balancers can be placed on the private-ingress subnet which can be made routable across an enterprise shared network as needed, This subnet would also have private nat-gateways with routing table rules that result in the primary VPC cidr being effectively non-routable. cidr ranges should not overlap amongst clusters of the same role, but the cidr space of a role in general could overlap with other roles since all traffic out of the cluster is nated, with private or public, and all inbound traffic is only through LBs in either the private or public ingress subnet.  

| vpc                     | region          | az                | az                | az                |
|-------------------------|:---------------:|:-----------------:|:-----------------:|:-----------------:|
|                         |                 |                   |                   |                   |
|                         | us-east-1       | us-east-1a        |   us-east-1b      |  us-east-1c       |
| sbx-i01-aws-us-east-1   | 10.80.0.0/16    |                   |                   |                   |
| private                 |                 | 10.80.0.0/18      | 10.80.64.0/18     | 10.80.128.0/18    |
| private-ingress         |                 | 10.80.0.0/18      | 10.80.64.0/18     | 10.80.128.0/18    |
| public-ingress          |                 | 10.80.240.0/26    | 10.80.240.64/26   | 10.80.240.128/26  |
| database                |                 | 10.80.192.0/20    | 10.80.208.0/20    | 10.80.224.0/20    |

Normally we would connect the various VPCs within a role through transit gateways, but in the live Labs platform we simplify for cost purposes.  

_terraform modules_  

Uses the following AWS official terraform modules: (check for updates)  

terraform-aws-modules/vpc/aws  

_use of circlecigen_  

You will see this pattern repeated in several of the foundation psk infrastructure pipelines. See the ADR for more detailed discussion of the tool.  

For this vpc pipeline, since we are limiting our use to a single sandbox and production network, use of the pipeline generation tool is overly complex. Were we to have multiple roles and instances the value becomes apparent. We are using here inspite of the low complexity for example purposes.  

### local engineering practice additions or notes (See lab-documentation for standard practices and ADRs)  

1. specific .gitignore entries

* Ignore all extensions of .auto.tfvars.json. These are autogenerated during the pipeline run and are ignored so that any local testing doesn't accidently cause problem to the pipeline.  
* Ignore all files that will be generated via the circlecigen pipeline continuation script. environments/*.tfvars*, .circleci/generated_config.yml
* Ignore the multi.json file fetched from 1password during the pipeline run.

1. additional pre-commit checks, these checks also done in the pipeline

* [tflint](https://github.com/terraform-linters/tflint)
* [tfsec](https://github.com/aquasecurity/tfsec)
* terraform fmt
* terraform validate
